{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MWGjHRX_7gF"
      },
      "source": [
        "# **Paper: Multi-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis** \n",
        "Authors: Wenhang Bao, Xiao-Yang Liu (Code available at : https://github.com/WenhangBao/MultiAgent-RL-for-Liquidation) \\\\\n",
        "presented by Janet Wang (yw4fm)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vojp4UZep2_R"
      },
      "source": [
        "Choose *ddpg_agent.py*, *model.py*, *syntheticChrissAlmgren.py*, *utilsgiven.py*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ0krp_a_7gF",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "edfa5007-0f55-45f7-98a7-16fce30128fd"
      },
      "source": [
        "#google drive\n",
        "import sys \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('utilsgiven.py','wb').write(src)\n",
        "import utilsgiven\n",
        "\n",
        "\n",
        "# Get the default financial and AC Model parameters\n",
        "financial_params, ac_params = utilsgiven.get_env_param()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0117e268-aaa6-4810-9bf7-9603bb2e9787\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0117e268-aaa6-4810-9bf7-9603bb2e9787\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ddpg_agent.py to ddpg_agent (1).py\n",
            "Saving model.py to model (1).py\n",
            "Saving syntheticChrissAlmgren.py to syntheticChrissAlmgren (2).py\n",
            "Saving utilsgiven.py to utilsgiven (2).py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awDtSU5UNfK5"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QUrdupb2k3i"
      },
      "source": [
        "We use the **Almgren-Chriss market impact model** defined in *syntheticChrissAlmgren.py* to solve the problem of finding an optimal liquidation strategy and experiment with multiple scenaios for the agents. In the context of reinforcement learning, the model serves as a trading environment where two agents make selling decisions and the environment returns price as information. \\\\\n",
        "To get started, we set up the **default financial environment** in *syntheticChrissAlmgren.py* by inputting the parameters below. These default parameters will remain the same across all experiments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKbN2y02_7gF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "outputId": "eeaf433d-9408-4f6f-b107-ddd3ea39ccdf"
      },
      "source": [
        "financial_params"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Financial Parameters</caption>\n",
              "<tr>\n",
              "  <th>Annual Volatility:</th>  <td>12%</td> <th>  Bid-Ask Spread:    </th>     <td>0.125</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Daily Volatility:</th>  <td>0.8%</td> <th>  Daily Trading Volume:</th> <td>5,000,000</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.table.SimpleTable'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW-p80_2Hct8"
      },
      "source": [
        "Then, we input parameters to the Almgren-Chriss market impact model to define a multi-agent scenario. Two agents are assumed to sell all shares within the given time frame.\n",
        "\n",
        "We redefine the following parameters everytime as we run a new experiment to get arrays of expected shortfalls and trajectories and keep the rest as default: \n",
        "\n",
        "1.   Total Number of Shares for Agent 1 to Sell\n",
        "2.   Total Number of Shares for Agent 2 to Sell\n",
        "3.   Trader's Risk Aversion for Agent 1\n",
        "4.   Trader's Risk Aversion for Agent 2\n",
        "\n",
        "For example, the following parameters are defined for two competitive agents each responsible for selling 0.5 million shares at risk aversion $\\lambda_{B_{1}}=\\lambda_{B_{2}}=1e^{-6}$ to explore the cooperation v. competition relationship in *visualization.ipynb*. All outputs below follow this example.\n",
        "\n",
        "We set \"*Total Number of Shares for Agent 2 to Sell*\" as 0.0001 to stimulate single-agent environment to set benchmark of comparison. The number of shares for Agent 2 must be greater than 0 to avoid division error in normalizatoin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwK_nxs_7gG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "82363306-1001-4623-c305-b1d77f19248e"
      },
      "source": [
        "ac_params"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Almgren and Chriss Model Parameters</caption>\n",
              "<tr>\n",
              "  <th>Total Number of Shares for Agent1 to Sell:</th>        <td>500,000</td> <th>  Fixed Cost of Selling per Share:</th>    <td>$0.062</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Total Number of Shares for Agent2 to Sell:</th>        <td>500,000</td> <th>  Trader's Risk Aversion for Agent 1:</th>  <td>1e-06</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Starting Price per Share:</th>                         <td>$50.00</td>  <th>  Trader's Risk Aversion for Agent 2:</th>  <td>1e-06</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Price Impact for Each 1% of Daily Volume Traded:</th> <td>$2.5e-06</td> <th>  Permanent Impact Constant:</th>          <td>2.5e-07</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Number of Days to Sell All the Shares:</th>              <td>60</td>    <th>  Single Step Variance:</th>                <td>0.144</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Number of Trades:</th>                                   <td>60</td>    <th>  Time Interval between trades:</th>         <td>1.0</td>  \n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.table.SimpleTable'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJtSmfH7YjbW"
      },
      "source": [
        "After setting up the environment, we model the liquidation process as ***MDP***. \\\n",
        "\n",
        "We adopt the ***Actor-Critic method*** that is parameterized with neural network to approximate both *Q-value* and action *a*. \n",
        "\n",
        "*  The **critic** learns the Q-value function. The critic network estimates the return *r* of the state and supplies knowledge of the performance to roll on the actions for Agent 1 and Agent 2. \n",
        "*  The **actor** updates the policy by information from critic. The actor **inputs** state *s* and return action *a* \n",
        "\n",
        "The paper referenced the **Deep Deterministic Policy Gradients (DDPG)** algorithm, which is an example of Actor-Critic method, to solve the optimal liquidation problem. Details are elaborated in *ddpg_agent.py*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44EqO50bYgd9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import syntheticChrissAlmgren as sca\n",
        "from ddpg_agent import Agent\n",
        "\n",
        "from collections import deque"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZYZxT4NKjyL"
      },
      "source": [
        "For each agent defined in *ddpg_agent.py*:\n",
        "\n",
        "*   Goal: minimizing expected shortfall (selling cost) \n",
        "*   State: [past return *r*, remaining number of trades *m*, remaining numebr of shares *l*]\n",
        "*   Reward: difference between utility functions before and after sale. Utility functions are defined defined by risk aversion level $\\lambda$ and trading trajectory (vector of shares remaning at each time step $k$)\n",
        "*   Action: sell(1) or hold(0) at each time step $k$ as well as market prices\n",
        "*   Policy: selling percentage *a* at state *s*\n",
        "*   Q(*s,a*): expected reward achieved by action *a* at state *s* \n",
        "\n",
        "Each agent only observes limited state infoormation as they only their own remaining shares, but not others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX5g5-fh_7gG",
        "scrolled": false
      },
      "source": [
        "# Create simulation environment\n",
        "env = sca.MarketEnvironment()\n",
        "\n",
        "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
        "agent1 = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(),random_seed = 1225)\n",
        "agent2 = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(),random_seed = 108)\n",
        "# Set the liquidation time\n",
        "lqt = 60\n",
        "\n",
        "# Set the number of trades\n",
        "n_trades = 60\n",
        "\n",
        "# Set trader's risk aversion\n",
        "tr1 = 1e-6\n",
        "tr2 = 1e-6"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLy4D-J6SQri"
      },
      "source": [
        "We then implement a reinforcement learning workflow to train the actor and crtic. We use DDPG algorithm to generate the optimal trading trajectory with minimum selling cost. Then, we feed in the states observed from the environment to Agent 1 and Agent 2. These two agents first performance based on these predicted actions. After the first round, the environment gains information and returns their new rewards and states. This process continues for 1,300 episodes in this case. \n",
        "\n",
        "For every 100 episode, we output the average shortfall of the two agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZLE8BuZ3DF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4bdb104-fa83-49f7-e10a-dc56eeb939f5"
      },
      "source": [
        "# Set the number of episodes to run the simulation\n",
        "episodes = 1300\n",
        "shortfall_list = []\n",
        "shortfall_hist1 = np.array([])\n",
        "shortfall_hist2 = np.array([])\n",
        "shortfall_deque1 = deque(maxlen=100)\n",
        "shortfall_deque2 = deque(maxlen=100)\n",
        "for episode in range(episodes): \n",
        "    # Reset the enviroment\n",
        "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb1 = tr1,lamb2 = tr2)\n",
        "\n",
        "    # set the environment to make transactions\n",
        "    env.start_transactions()\n",
        "\n",
        "    for i in range(n_trades + 1):\n",
        "      \n",
        "        # Predict the best action for the current state based on DDPG. \n",
        "        cur_state1 = np.delete(cur_state,8)\n",
        "        cur_state2 = np.delete(cur_state,7)\n",
        "       \n",
        "        action1 = agent1.act(cur_state1, add_noise = True)\n",
        "        action2 = agent2.act(cur_state2, add_noise = True)\n",
        "        \n",
        "        # Action is performed and new state, reward, info are received. \n",
        "        new_state, reward1, reward2, done1, done2, info = env.step(action1,action2)\n",
        "        \n",
        "        # current state, action, reward, new state are stored in the experience replay\n",
        "        new_state1 = np.delete(new_state,8)\n",
        "        new_state2 = np.delete(new_state,7)\n",
        "        agent1.step(cur_state1, action1, reward1, new_state1, done1)\n",
        "        agent2.step(cur_state2, action2, reward2, new_state2, done2)\n",
        "        # roll over new state\n",
        "        cur_state = new_state\n",
        "\n",
        "        if info.done1 and info.done2:\n",
        "            shortfall_hist1 = np.append(shortfall_hist1, info.implementation_shortfall1)\n",
        "            shortfall_deque1.append(info.implementation_shortfall1)\n",
        "            \n",
        "            shortfall_hist2 = np.append(shortfall_hist2, info.implementation_shortfall2)\n",
        "            shortfall_deque2.append(info.implementation_shortfall2)\n",
        "            break\n",
        "        \n",
        "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
        "        print('\\rEpisode [{}/{}]\\tAverage Shortfall for Agent1: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque1)))        \n",
        "        print('\\rEpisode [{}/{}]\\tAverage Shortfall for Agent2: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque2)))\n",
        "        shortfall_list.append([np.mean(shortfall_deque1),np.mean(shortfall_deque2)])\n",
        "print('\\nAverage Implementation Shortfall for Agent1: ${:,.2f} \\n'.format(np.mean(shortfall_hist1)))\n",
        "print('\\nAverage Implementation Shortfall for Agent2: ${:,.2f} \\n'.format(np.mean(shortfall_hist2)))\n",
        "#utilsgiven.plot_price_model()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpisode [100/1300]\tAverage Shortfall for Agent1: $342,037.37\n",
            "Episode [100/1300]\tAverage Shortfall for Agent2: $343,009.38\n",
            "Episode [200/1300]\tAverage Shortfall for Agent1: $321,481.76\n",
            "Episode [200/1300]\tAverage Shortfall for Agent2: $321,917.95\n",
            "Episode [300/1300]\tAverage Shortfall for Agent1: $318,075.91\n",
            "Episode [300/1300]\tAverage Shortfall for Agent2: $317,478.14\n",
            "Episode [400/1300]\tAverage Shortfall for Agent1: $371,157.72\n",
            "Episode [400/1300]\tAverage Shortfall for Agent2: $373,268.12\n",
            "Episode [500/1300]\tAverage Shortfall for Agent1: $337,774.91\n",
            "Episode [500/1300]\tAverage Shortfall for Agent2: $343,546.48\n",
            "Episode [600/1300]\tAverage Shortfall for Agent1: $347,867.41\n",
            "Episode [600/1300]\tAverage Shortfall for Agent2: $348,298.31\n",
            "Episode [700/1300]\tAverage Shortfall for Agent1: $302,789.39\n",
            "Episode [700/1300]\tAverage Shortfall for Agent2: $296,596.55\n",
            "Episode [800/1300]\tAverage Shortfall for Agent1: $305,151.05\n",
            "Episode [800/1300]\tAverage Shortfall for Agent2: $301,542.19\n",
            "Episode [900/1300]\tAverage Shortfall for Agent1: $343,508.22\n",
            "Episode [900/1300]\tAverage Shortfall for Agent2: $342,052.92\n",
            "Episode [1000/1300]\tAverage Shortfall for Agent1: $318,731.56\n",
            "Episode [1000/1300]\tAverage Shortfall for Agent2: $317,495.71\n",
            "Episode [1100/1300]\tAverage Shortfall for Agent1: $329,135.85\n",
            "Episode [1100/1300]\tAverage Shortfall for Agent2: $333,255.71\n",
            "Episode [1200/1300]\tAverage Shortfall for Agent1: $300,993.44\n",
            "Episode [1200/1300]\tAverage Shortfall for Agent2: $301,320.57\n",
            "Episode [1300/1300]\tAverage Shortfall for Agent1: $294,413.69\n",
            "Episode [1300/1300]\tAverage Shortfall for Agent2: $292,937.04\n",
            "\n",
            "Average Implementation Shortfall for Agent1: $325,624.48 \n",
            "\n",
            "\n",
            "Average Implementation Shortfall for Agent2: $325,593.77 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q4VHh8tMYv1"
      },
      "source": [
        "We save the shortfall list of both Agent 1 and Agent 2 as a .npy file to input in visualization.ipynb. The output below is an example of the first few lines of *\"1e-6_1e-6_cooporation_shorfall_list.npy\"* when Agent 1 and Agent 2 are in corporative relationships trading at $\\lambda_{1} = \\lambda_{2} = 1e^{-6}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHymCzH-_7gG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3c7e0c-dca6-4ebf-872a-b71df8aa9830"
      },
      "source": [
        "shortfall = np.array(shortfall_list)\n",
        "print(shortfall[0:5])\n",
        "np.save('1e-6_1e-6_cooporation_shorfall_list.npy',shortfall)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1168737.12304209 1182497.03935972]\n",
            " [1281250.         1281250.        ]\n",
            " [1274753.8617609  1278818.425     ]\n",
            " [ 958446.85059046  996403.32558362]\n",
            " [ 321537.15559445  321944.70308262]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwSIJ-pIMde8"
      },
      "source": [
        "The output below is the shares remaining at each step time $k$, also known as the trading trajectory of both agents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnOqpyfF_7gH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8192a368-b0be-4cb8-e761-0b77bf88191f"
      },
      "source": [
        "#print(tr1,tr2)\n",
        "cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb1 = tr1,lamb2 = tr2)\n",
        "\n",
        "    # set the environment to make transactions\n",
        "env.start_transactions()\n",
        "\n",
        "trajectory = np.zeros([n_trades+1,2])\n",
        "for i in range(n_trades + 1):\n",
        "    trajectory[i] = cur_state[7:]\n",
        "    \n",
        "    print(cur_state[7:])\n",
        "        # Predict the best action for the current state. \n",
        "    cur_state1 = np.delete(cur_state,8)\n",
        "    cur_state2 = np.delete(cur_state,7)\n",
        "        #print(cur_state[5:])\n",
        "    action1 = agent1.act(cur_state1, add_noise = True)\n",
        "    action2 = agent2.act(cur_state2, add_noise = True)\n",
        "        #print(action1,action2)\n",
        "        # Action is performed and new state, reward, info are received. \n",
        "    new_state, reward1, reward2, done1, done2, info = env.step(action1,action2)\n",
        "        \n",
        "        # current state, action, reward, new state are stored in the experience replay\n",
        "    new_state1 = np.delete(new_state,8)\n",
        "    new_state2 = np.delete(new_state,7)\n",
        "    agent1.step(cur_state1, action1, reward1, new_state1, done1)\n",
        "    agent2.step(cur_state2, action2, reward2, new_state2, done2)\n",
        "        # roll over new state\n",
        "    cur_state = new_state\n",
        "\n",
        "    if info.done1 and info.done2:\n",
        "        shortfall_hist1 = np.append(shortfall_hist1, info.implementation_shortfall1)\n",
        "        shortfall_deque1.append(info.implementation_shortfall1)\n",
        "            \n",
        "        shortfall_hist2 = np.append(shortfall_hist2, info.implementation_shortfall2)\n",
        "        shortfall_deque2.append(info.implementation_shortfall2)\n",
        "        break\n",
        "        \n",
        "if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
        "    print('\\rEpisode [{}/{}]\\tAverage Shortfall for Agent1: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque1)))        \n",
        "    print('\\rEpisode [{}/{}]\\tAverage Shortfall for Agent2: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque2)))\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1e-06 1e-06\n",
            "[1. 1.]\n",
            "[0.761694 0.656324]\n",
            "[0.603648 0.454928]\n",
            "[0.44365  0.334226]\n",
            "[0.305346 0.25539 ]\n",
            "[0.20247  0.202642]\n",
            "[0.13316  0.148788]\n",
            "[0.09197 0.10399]\n",
            "[0.064072 0.074902]\n",
            "[0.044238 0.052522]\n",
            "[0.03257  0.036602]\n",
            "[0.02397  0.024466]\n",
            "[0.018556 0.01732 ]\n",
            "[0.013314 0.011942]\n",
            "[0.009696 0.008204]\n",
            "[0.006774 0.005622]\n",
            "[0.004728 0.003898]\n",
            "[0.003236 0.002704]\n",
            "[0.00228  0.001762]\n",
            "[0.00165 0.00114]\n",
            "[0.001234 0.000778]\n",
            "[0.000932 0.000566]\n",
            "[0.000674 0.000392]\n",
            "[0.000506 0.00029 ]\n",
            "[0.000376 0.000212]\n",
            "[0.000294 0.00015 ]\n",
            "[0.000224 0.000108]\n",
            "[1.66e-04 7.40e-05]\n",
            "[1.14e-04 5.40e-05]\n",
            "[8.2e-05 4.2e-05]\n",
            "[5.8e-05 3.4e-05]\n",
            "[4.0e-05 2.8e-05]\n",
            "[2.6e-05 2.2e-05]\n",
            "[1.8e-05 1.8e-05]\n",
            "[1.2e-05 1.4e-05]\n",
            "[8.e-06 1.e-05]\n",
            "[6.e-06 8.e-06]\n",
            "[4.e-06 6.e-06]\n",
            "[2.e-06 4.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "[2.e-06 2.e-06]\n",
            "Episode [1300/1300]\tAverage Shortfall for Agent1: $298,868.72\n",
            "Episode [1300/1300]\tAverage Shortfall for Agent2: $296,739.36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9LLUF-ABcU9"
      },
      "source": [
        "We save the trajectory of both Agent 1 and Agent 2 as a .npy file to input in visualization.ipynb later. The output below is an example of the first few lines of \"*1e-6_1e-6_competition_shorfall_list.npy*\" when Agent 1 and Agent 2 are in competitive relationships trading at $\\lambda_{1} = \\lambda_{2} = 1e^{-6}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFYyhaD2_7gH"
      },
      "source": [
        "np.save('1e-6_1e-6_competition_trajectory_1500.npy',trajectory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2IJ6i40C57r"
      },
      "source": [
        "We will have to repeat the above several times by altering the ac_params in *syntheticChrissAlmgren.py* to generate .npy files of shortfall lists and trajectory lists. Here's a summary of the parameters that require changes and corresponding files generated categorized by experiments: \n",
        "\n",
        "Theorem 1ï¼š\n",
        "\n",
        "| File name | Total Number of Shares for Agent1 to Sell |Total Number of Shares for Agent2 to Sell | risk level Agent1| risk level Agent2\n",
        "| :- | :-: | :-: |:-: |:-: |\n",
        "| *1e-6_shortfall_list.npy*  | 1,000,000 | 0.0001| 1e^{-6}| 0\n",
        "| *1e-6_shortfall_list 0.3M.npy*   | 300,000 | 700,000| 1e^{-6}|1e^{-6}\n",
        "| *1e-6_shortfall_list 0.7M.npy*  | 700,000 | 300,000| 1e^{-6}|1e^{-6}\n",
        "\n",
        "\n",
        "Theorem 2:\n",
        "\n",
        "| File name | Total Number of Shares for Agent1 to Sell |Total Number of Shares for Agent2 to Sell | risk level Agent1| risk level Agent2\n",
        "| :- | :-: | :-: |:-: |:-: |\n",
        "| *1e-6_shortfall_optimal.npy*   | 1,000,000 | 0.0001| 1e^{-4}| 0\n",
        "| *1e-9_shortfall_optimal.npy*   | 0.0001 |1,000,000| 0|1e^{-9}\n",
        "| *1e-4_le-9_trajectory.npy*   | 500,000 | 500,000| 1e^{-4}|1e^{-9}\n",
        "\n",
        "Competition v. Corperation: \n",
        "\n",
        "| File name | Total Number of Shares for Agent1 to Sell |Total Number of Shares for Agent2 to Sell | risk level Agent1| risk level Agent2|reward|episodes\n",
        "| :- | :-: | :-: |:-: |:-: |:-: |:-: |\n",
        "| *1e-6_shortfall_list.npy*   | 1,000,000 | 0.0001| 1e^{-6}| 0\n",
        "| *1e-6_le-6_competition_shortfall_list.npy*   | 500,000 | 500,000| 1e^{-6}| 1e^{-6}|competitive|1300\n",
        "| *1e-6_le-6_corporatition_shortfall_list.npy*  | 500,000 | 500,000| 1e^{-6}| 1e^{-6}|corporative|1300\n",
        "| *1e-6_le-6_competition_trajectory_1500.npy*  | 500,000 | 500,000| 1e^{-6}| 1e^{-6}|competition|1500\n",
        "\n",
        "\n",
        "Liquidation Optimal Strategy:\n",
        "1.  *1e-6_optimal.npy*\n",
        "2.  *1e-6_trajectory_fixed-competitor.npy*\n",
        "3.  *1e-6_trajectory_fixed-corporation.npy*\n",
        "\n",
        "| File name | Total Number of Shares for Agent1 to Sell |Total Number of Shares for Agent2 to Sell | risk level Agent1| risk level Agent2|reward|episodes\n",
        "| :- | :-: | :-: |:-: |:-: |:-: |:-: |\n",
        "| *1e-6_optimal.npy*   | 1,000,000 | 0.0001| 1e^{-6}| 0\n",
        "| *1e-6_trajectory_fixed-competitor.npy*   | 500,000 | 500,000| 1e^{-6}| 1e^{-6}|competitive|1300\n",
        "| *1e-6_trajectory_fixed-corporation.npy*  | 500,000 | 500,000| 1e^{-6}| 1e^{-6}|corporative|1300\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCLX2XM-wFUN"
      },
      "source": [
        "After getting all the files in the tables above, we will move on to *visualization.ipynb* to analyze and interpret the results."
      ]
    }
  ]
}